{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "gOkeHNdKt6vx",
        "outputId": "5573cbb6-7308-4154-c01d-a889f4d6ae92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "# Try optional imports\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    has_xgb = True\n",
        "except Exception:\n",
        "    has_xgb = False\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "    has_tf = True\n",
        "except Exception:\n",
        "    has_tf = False\n",
        "\n",
        "DATA_PATH = \"/content/bangalore_parking_6months.csv\"  # place file in same folder or change path\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}\"\n",
        "\n",
        "# 1) Load and basic preprocessing\n",
        "df = pd.read_csv(DATA_PATH) # Removed parse_dates here\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp']) # Explicit conversion\n",
        "df = df.sort_values([\"loc_id\",\"timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "# time features\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['minute'] = df['timestamp'].dt.minute\n",
        "df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
        "df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "target_col = 'occupancy_rate'  # between 0 and 1\n",
        "\n",
        "# 2) Create lag features (in 15-min steps)\n",
        "lag_steps = [1,4,96]  # 15min, 1h, 24h\n",
        "for lag in lag_steps:\n",
        "    df[f'lag_{lag}'] = df.groupby('loc_id')[target_col].shift(lag)\n",
        "\n",
        "# Rolling statistics\n",
        "df['roll_mean_4'] = df.groupby('loc_id')[target_col].shift(1).rolling(window=4, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "df['roll_mean_96'] = df.groupby('loc_id')[target_col].shift(1).rolling(window=96, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "# Fill missing lag/rolls with loc mean\n",
        "df['loc_mean'] = df.groupby('loc_id')[target_col].transform('mean')\n",
        "for c in [f'lag_{l}' for l in lag_steps] + ['roll_mean_4','roll_mean_96']:\n",
        "    df[c] = df[c].fillna(df['loc_mean'])\n",
        "\n",
        "\n",
        "# Label-encode loc_id\n",
        "le = LabelEncoder()\n",
        "df['loc_enc'] = le.fit_transform(df['loc_id'])\n",
        "\n",
        "# Features for tree model\n",
        "feature_cols = ['loc_enc','capacity','hour','dayofweek','is_weekend',\n",
        "                'lag_1','lag_4','lag_96','roll_mean_4','roll_mean_96','rain','is_event']\n",
        "\n",
        "# Drop any remaining NaNs after feature engineering\n",
        "# This ensures clean data for model training and evaluation\n",
        "initial_rows = df.shape[0]\n",
        "df.dropna(subset=[target_col] + feature_cols, inplace=True)\n",
        "if df.shape[0] < initial_rows:\n",
        "    print(f\"Dropped {initial_rows - df.shape[0]} rows with NaNs after feature engineering.\")\n",
        "\n",
        "# 3) Time-based splits (example: train May-Jul, val Aug, test Sep)\n",
        "df['month'] = df['timestamp'].dt.month\n",
        "train_df = df[df['month']==5] # May\n",
        "val_df = df[df['month']==6] # June\n",
        "test_df = df[df['month']==7] # July\n",
        "\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[target_col].values\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df[target_col].values\n",
        "X_test = test_df[feature_cols].values\n",
        "y_test = test_df[target_col].values\n",
        "\n",
        "# 4) Scale continuous features\n",
        "cont_cols = ['capacity','lag_1','lag_4','lag_96','roll_mean_4','roll_mean_96']\n",
        "cont_idx = [feature_cols.index(c) for c in cont_cols]\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[:, cont_idx])\n",
        "\n",
        "def scale_X(X):\n",
        "    Xs = X.copy().astype(float)\n",
        "    Xs[:, cont_idx] = scaler.transform(Xs[:, cont_idx])\n",
        "    return Xs\n",
        "\n",
        "X_train_s = scale_X(X_train)\n",
        "X_val_s = scale_X(X_val)\n",
        "X_test_s = scale_X(X_test)\n",
        "\n",
        "# Save label encoder and scaler\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "joblib.dump(le, \"artifacts/loc_label_encoder.joblib\")\n",
        "joblib.dump(scaler, \"artifacts/feature_scaler.joblib\")\n",
        "\n",
        "# 5) Train XGBoost (or fallback HGB)\n",
        "if has_xgb:\n",
        "    dtrain = xgb.DMatrix(X_train_s, label=y_train, feature_names=feature_cols)\n",
        "    dval = xgb.DMatrix(X_val_s, label=y_val, feature_names=feature_cols)\n",
        "    params = {\n",
        "        \"objective\":\"reg:squarederror\",\n",
        "        \"eval_metric\":\"rmse\",\n",
        "        \"tree_method\":\"hist\",\n",
        "        \"learning_rate\":0.1,\n",
        "        \"max_depth\":8,\n",
        "        \"subsample\":0.8,\n",
        "        \"colsample_bytree\":0.8,\n",
        "        \"seed\":42\n",
        "    }\n",
        "    xgb_model = xgb.train(params, dtrain, num_boost_round=300,\n",
        "                          evals=[(dtrain,'train'),(dval,'val')],\n",
        "                          early_stopping_rounds=20, verbose_eval=20)\n",
        "    xgb_model.save_model(\"artifacts/xgb_parking_model.json\")\n",
        "    y_pred_xgb = xgb_model.predict(xgb.DMatrix(X_test_s, feature_names=feature_cols))\n",
        "else:\n",
        "    print(\"xgboost not found — using sklearn HistGradientBoostingRegressor fallback\")\n",
        "    hgb = HistGradientBoostingRegressor(max_iter=300, learning_rate=0.05, max_depth=10, random_state=42)\n",
        "    hgb.fit(X_train_s, y_train)\n",
        "    joblib.dump(hgb, \"artifacts/hgb_parking_model.joblib\")\n",
        "    y_pred_xgb = hgb.predict(X_test_s)\n",
        "\n",
        "# Evaluate XGB/HGB\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "print(\"XGBoost/HGB Results — MAE: {:.4f}, RMSE: {:.4f}, R2: {:.4f}\".format(mae_xgb, rmse_xgb, r2_xgb))\n",
        "\n",
        "# Save sample predictions\n",
        "test_out = test_df[['timestamp','loc_id','occupancy_rate']].copy()\n",
        "test_out['pred_xgb'] = y_pred_xgb\n",
        "test_out.to_csv(\"artifacts/test_predictions_xgb_sample.csv\", index=False)\n",
        "\n",
        "# Initialize LSTM metrics\n",
        "mae_lstm = None\n",
        "rmse_lstm = None\n",
        "r2_lstm = None\n",
        "\n",
        "# 6) LSTM pipeline (OPTIONAL — requires TensorFlow)\n",
        "if not has_tf:\n",
        "    print(\"TensorFlow not installed. Skipping LSTM training. To run the LSTM, install tensorflow and re-run.\")\n",
        "else:\n",
        "    # We'll train a demo LSTM on a subset of locations (to keep runtime reasonable)\n",
        "    sample_locs = df['loc_id'].unique()[:8]  # choose 8 locs\n",
        "    lstm_df = df[df['loc_id'].isin(sample_locs)].copy()\n",
        "    period_start = datetime(2025,7,1)\n",
        "    period_end = period_start + pd.Timedelta(days=60) - pd.Timedelta(minutes=15)\n",
        "    lstm_df = lstm_df[(lstm_df['timestamp']>=period_start) & (lstm_df['timestamp']<=period_end)]\n",
        "\n",
        "    window = 96  # 24h\n",
        "    X_seqs = []\n",
        "    y_seqs = []\n",
        "    for loc in lstm_df['loc_id'].unique():\n",
        "        sub = lstm_df[lstm_df['loc_id']==loc].sort_values('timestamp')\n",
        "        vals = sub[target_col].values\n",
        "        hours = sub['hour'].values.reshape(-1,1)\n",
        "        weekends = sub['is_weekend'].values.reshape(-1,1)\n",
        "        features = np.concatenate([vals.reshape(-1,1), hours, weekends], axis=1)\n",
        "        for i in range(len(features)-window-1):\n",
        "            X_seqs.append(features[i:i+window])\n",
        "            y_seqs.append(features[i+window,0])\n",
        "\n",
        "    X_seqs = np.array(X_seqs); y_seqs = np.array(y_seqs)\n",
        "    n = len(X_seqs); train_n = int(n*0.8); val_n = int(n*0.1)\n",
        "    X_tr, X_va, X_te = X_seqs[:train_n], X_seqs[train_n:train_n+val_n], X_seqs[train_n+val_n:]\n",
        "    y_tr, y_va, y_te = y_seqs[:train_n], y_seqs[train_n:train_n+val_n], y_seqs[train_n+val_n:]\n",
        "\n",
        "    tf.random.set_seed(42)\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(window, X_tr.shape[-1]), return_sequences=False),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    # Train (adjust epochs/batch_size to your GPU/CPU resources)\n",
        "    history = model.fit(X_tr, y_tr, validation_data=(X_va,y_va), epochs=6, batch_size=128, verbose=2)\n",
        "\n",
        "    y_pred_lstm = model.predict(X_te).flatten()\n",
        "    mae_lstm = mean_absolute_error(y_te, y_pred_lstm)\n",
        "    rmse_lstm = np.sqrt(mean_squared_error(y_te, y_pred_lstm))\n",
        "    r2_lstm = r2_score(y_te, y_pred_lstm)\n",
        "    print(\"LSTM demo results — MAE: {:.4f}, RMSE: {:.4f}, R2: {:.4f}\".format(mae_lstm, rmse_lstm, r2_lstm))\n",
        "\n",
        "    model.save(\"artifacts/lstm_parking_demo_model.keras\")\n",
        "    pd.DataFrame({\"y_true\":y_te[:200], \"y_pred\":y_pred_lstm[:200]}).to_csv(\"artifacts/lstm_test_sample.csv\", index=False)\n",
        "\n",
        "# 7) Save run summary\n",
        "summary_lstm_mae = None\n",
        "summary_lstm_rmse = None\n",
        "summary_lstm_r2 = None\n",
        "\n",
        "if has_tf:\n",
        "    if mae_lstm is not None:\n",
        "        summary_lstm_mae = float(mae_lstm)\n",
        "    if rmse_lstm is not None:\n",
        "        summary_lstm_rmse = float(rmse_lstm)\n",
        "    if r2_lstm is not None:\n",
        "        summary_lstm_r2 = float(r2_lstm)\n",
        "\n",
        "summary = {\n",
        "    \"xgb\": {\"mae\": float(mae_xgb), \"rmse\": float(rmse_xgb), \"r2\": float(r2_xgb)},\n",
        "    \"lstm_demo\": {\n",
        "        \"mae\": summary_lstm_mae,\n",
        "        \"rmse\": summary_lstm_rmse,\n",
        "        \"r2\": summary_lstm_r2\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"artifacts/run_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Artifacts saved to artifacts/ directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBWzRn2DvQui",
        "outputId": "1c200564-7b6c-4d6a-951f-c9ca23523994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nearest synthetic loc_id to Cool Joint: L002 with distance: 0.0012899352696991244\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "Prediction for Cool Joint (approx) at coordinates: 12.9289 77.585\n",
            "Using synthetic location: L002\n",
            "XGBoost prediction (occupancy_rate): 0.658\n",
            "LSTM prediction (occupancy_rate): 0.727\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from xgboost import XGBRegressor\n",
        "from tensorflow.keras.models import load_model\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "COOL_JOINT_LAT = 12.9289\n",
        "COOL_JOINT_LON = 77.5850\n",
        "\n",
        "# Paths to saved models/scalers\n",
        "XGB_MODEL_PATH = \"artifacts/xgb_parking_model.json\"\n",
        "SCALER_PATH = \"artifacts/feature_scaler.joblib\"\n",
        "LABEL_ENCODER_PATH = \"artifacts/loc_label_encoder.joblib\"\n",
        "\n",
        "LSTM_MODEL_PATH = \"artifacts/lstm_parking_demo_model.keras\"           # if saved as TF SavedModel\n",
        "# or: LSTM_MODEL_PATH = \"artifacts/lstm_parking_model.h5\" if .h5\n",
        "\n",
        "# Your data\n",
        "DATA_PATH = \"bangalore_parking_6months.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH) # Removed parse_dates here\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp']) # Explicit conversion\n",
        "df = df.sort_values([\"loc_id\", \"timestamp\"])\n",
        "\n",
        "# --- Re-create feature engineering for df ---\n",
        "target_col = 'occupancy_rate'\n",
        "\n",
        "# time features\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['minute'] = df['timestamp'].dt.minute\n",
        "df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
        "df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "# Create lag features (in 15-min steps)\n",
        "lag_steps = [1,4,96]  # 15min, 1h, 24h\n",
        "for lag in lag_steps:\n",
        "    df[f'lag_{lag}'] = df.groupby('loc_id')[target_col].shift(lag)\n",
        "\n",
        "# Rolling statistics\n",
        "df['roll_mean_4'] = df.groupby('loc_id')[target_col].shift(1).rolling(window=4, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "df['roll_mean_96'] = df.groupby('loc_id')[target_col].shift(1).rolling(window=96, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "# Fill missing lag/rolls with loc mean\n",
        "df['loc_mean'] = df.groupby('loc_id')[target_col].transform('mean')\n",
        "for c in [f'lag_{l}' for l in lag_steps] + ['roll_mean_4','roll_mean_96']:\n",
        "    df[c] = df[c].fillna(df['loc_mean'])\n",
        "# --- End re-create feature engineering ---\n",
        "\n",
        "xgb_model = XGBRegressor()\n",
        "xgb_model.load_model(XGB_MODEL_PATH)\n",
        "\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "le = joblib.load(LABEL_ENCODER_PATH)\n",
        "\n",
        "lstm_model = None\n",
        "try:\n",
        "    lstm_model = load_model(LSTM_MODEL_PATH)\n",
        "    has_lstm = True\n",
        "except Exception as e:\n",
        "    print(\"LSTM model not loaded:\", e)\n",
        "    has_lstm = False\n",
        "\n",
        "# Compute distance to all locations (Euclidean approx)\n",
        "locs = df.groupby(\"loc_id\")[[\"lat\",\"lon\"]].first().reset_index()\n",
        "locs[\"dist\"] = np.sqrt((locs[\"lat\"] - COOL_JOINT_LAT)**2 + (locs[\"lon\"] - COOL_JOINT_LON)**2)\n",
        "nearest = locs.sort_values(\"dist\").iloc[0]\n",
        "loc_id = nearest[\"loc_id\"]\n",
        "print(\"Nearest synthetic loc_id to Cool Joint:\", loc_id, \"with distance:\", nearest[\"dist\"])\n",
        "\n",
        "# Filter df for that loc_id\n",
        "loc_df = df[df[\"loc_id\"] == loc_id].copy().reset_index(drop=True)\n",
        "\n",
        "# Feature-engineering for the latest timestamp\n",
        "latest = loc_df.iloc[-1].copy()\n",
        "latest_hour = latest[\"timestamp\"].hour\n",
        "latest_dow = latest[\"timestamp\"].weekday()\n",
        "latest[\"hour\"] = latest_hour\n",
        "latest[\"dayofweek\"] = latest_dow\n",
        "latest[\"is_weekend\"] = 1 if latest_dow >= 5 else 0\n",
        "\n",
        "feat_cols = ['loc_enc','capacity','hour','dayofweek','is_weekend',\n",
        "             'lag_1','lag_4','lag_96','roll_mean_4','roll_mean_96','rain','is_event']\n",
        "\n",
        "# Encode loc_enc\n",
        "latest[\"loc_enc\"] = le.transform([latest[\"loc_id\"]])[0]\n",
        "\n",
        "xgb_input = latest[feat_cols].values.reshape(1, -1).astype(float)\n",
        "# Scale continuous features: need to scale exactly as during training\n",
        "# Let's assume capacity, lag_*, roll_mean_* were scaled\n",
        "cont_idx = [feat_cols.index(c) for c in ['capacity','lag_1','lag_4','lag_96','roll_mean_4','roll_mean_96']]\n",
        "xgb_input[:, cont_idx] = scaler.transform(xgb_input[:, cont_idx])\n",
        "\n",
        "pred_xgb = xgb_model.predict(xgb_input)[0]\n",
        "\n",
        "pred_lstm = None\n",
        "if has_lstm:\n",
        "    # Prepare sequence: take last N occupancy points\n",
        "    window = 96  # 24h if using 15-min steps\n",
        "    seq = loc_df[\"occupancy_rate\"].values[-window:]\n",
        "    if len(seq) < window:\n",
        "        raise ValueError(\"Not enough history for LSTM prediction\")\n",
        "\n",
        "    # Also include exogenous like hour and weekend flag\n",
        "    hours = loc_df[\"hour\"].values[-window:]\n",
        "    weekends = loc_df[\"is_weekend\"].values[-window:]\n",
        "\n",
        "    features = np.column_stack([seq, hours, weekends])\n",
        "    features = features.reshape(1, window, features.shape[1])\n",
        "\n",
        "    # If you had scaled for LSTM\n",
        "    # (If you used a scaler for LSTM training, load and apply it here.)\n",
        "    # For now, assuming raw values\n",
        "    pred_lstm = lstm_model.predict(features)[0][0]\n",
        "\n",
        "print(\"Prediction for Cool Joint (approx) at coordinates:\", COOL_JOINT_LAT, COOL_JOINT_LON)\n",
        "print(\"Using synthetic location:\", loc_id)\n",
        "print(\"XGBoost prediction (occupancy_rate):\", round(pred_xgb, 3))\n",
        "if has_lstm:\n",
        "    print(\"LSTM prediction (occupancy_rate):\", round(pred_lstm, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Be-eIGC1cg",
        "outputId": "17448222-11ca-4540-bd5c-1a7ff194f956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nearest synthetic loc_id to Clarence Public School: L004 distance: 0.008076448786450918\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "Predicted occupancy for Clarence Public School (JP Nagar 4th Phase)\n",
            "Synthetic loc_id used: L004\n",
            "XGBoost occupancy_rate: 0.665\n",
            "LSTM occupancy_rate: 0.684\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from xgboost import XGBRegressor\n",
        "from tensorflow.keras.models import load_model\n",
        "from datetime import datetime\n",
        "\n",
        "CLARENCE_LAT = 12.9250  # <— approximate latitude\n",
        "CLARENCE_LON = 77.5760  # <— approximate longitude\n",
        "\n",
        "# Paths to your saved models / scaler\n",
        "XGB_MODEL_PATH = \"artifacts/xgb_parking_model.json\"\n",
        "SCALER_PATH = \"artifacts/feature_scaler.joblib\"\n",
        "LABEL_ENCODER_PATH = \"artifacts/loc_label_encoder.joblib\"\n",
        "\n",
        "LSTM_MODEL_PATH = \"artifacts/lstm_parking_demo_model.keras\"  # or .h5 path, as per your setup\n",
        "\n",
        "# Data\n",
        "DATA_PATH = \"bangalore_parking_6months.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH, parse_dates=[\"timestamp\"])\n",
        "df = df.sort_values([\"loc_id\",\"timestamp\"])\n",
        "\n",
        "# Re-create feature engineering for df\n",
        "target_col = 'occupancy_rate'\n",
        "\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['minute'] = df['timestamp'].dt.minute\n",
        "df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
        "df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "lag_steps = [1,4,96]  # 15min, 1h, 24h\n",
        "for lag in lag_steps:\n",
        "    df[f'lag_{lag}'] = df.groupby('loc_id')[target_col].shift(lag)\n",
        "\n",
        "df['roll_mean_4'] = df.groupby('loc_id')[target_col].shift(1).rolling(window=4, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "df['roll_mean_96'] = df.groupby('loc_id')[target_col].shift(1).rolling(window=96, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "df['loc_mean'] = df.groupby('loc_id')[target_col].transform('mean')\n",
        "for c in [f'lag_{l}' for l in lag_steps] + ['roll_mean_4','roll_mean_96']:\n",
        "    df[c] = df[c].fillna(df['loc_mean'])\n",
        "\n",
        "# Load models / scaler\n",
        "xgb_model = XGBRegressor()\n",
        "xgb_model.load_model(XGB_MODEL_PATH)\n",
        "\n",
        "scaler = joblib.load(SCALER_PATH)\n",
        "le = joblib.load(LABEL_ENCODER_PATH)\n",
        "\n",
        "try:\n",
        "    lstm_model = load_model(LSTM_MODEL_PATH)\n",
        "    has_lstm = True\n",
        "except Exception as e:\n",
        "    print(\"Could not load LSTM:\", e)\n",
        "    has_lstm = False\n",
        "\n",
        "locs = df.groupby(\"loc_id\")[[\"lat\", \"lon\"]].first().reset_index()\n",
        "locs[\"dist\"] = np.sqrt((locs[\"lat\"] - CLARENCE_LAT)**2 + (locs[\"lon\"] - CLARENCE_LON)**2)\n",
        "nearest = locs.sort_values(\"dist\").iloc[0]\n",
        "loc_id = nearest[\"loc_id\"]\n",
        "print(\"Nearest synthetic loc_id to Clarence Public School:\", loc_id, \"distance:\", nearest[\"dist\"])\n",
        "\n",
        "loc_df = df[df[\"loc_id\"] == loc_id].copy().reset_index(drop=True)\n",
        "\n",
        "latest = loc_df.iloc[-1].copy()\n",
        "latest[\"hour\"] = latest[\"timestamp\"].hour\n",
        "latest[\"dayofweek\"] = latest[\"timestamp\"].dayofweek\n",
        "latest[\"is_weekend\"] = 1 if latest[\"dayofweek\"] >= 5 else 0\n",
        "\n",
        "# Feature columns — should match training features\n",
        "feat_cols = ['loc_enc','capacity','hour','dayofweek','is_weekend',\n",
        "             'lag_1','lag_4','lag_96','roll_mean_4','roll_mean_96','rain','is_event']\n",
        "\n",
        "# Encode loc\n",
        "latest[\"loc_enc\"] = le.transform([latest[\"loc_id\"]])[0]\n",
        "\n",
        "xgb_input = latest[feat_cols].values.reshape(1, -1).astype(float)\n",
        "\n",
        "# Scale continuous features\n",
        "cont_cols = ['capacity','lag_1','lag_4','lag_96','roll_mean_4','roll_mean_96']\n",
        "cont_idx = [feat_cols.index(c) for c in cont_cols]\n",
        "xgb_input[:, cont_idx] = scaler.transform(xgb_input[:, cont_idx])\n",
        "\n",
        "pred_xgb = xgb_model.predict(xgb_input)[0]\n",
        "\n",
        "pred_lstm = None\n",
        "if has_lstm:\n",
        "    window = 96  # same window used in training\n",
        "    seq = loc_df[\"occupancy_rate\"].values[-window:]\n",
        "    if len(seq) < window:\n",
        "        raise ValueError(\"Not enough data for LSTM sequence\")\n",
        "\n",
        "    hours = loc_df[\"hour\"].values[-window:]\n",
        "    weekends = loc_df[\"is_weekend\"].values[-window:]\n",
        "    features = np.column_stack([seq, hours, weekends])\n",
        "    features = features.reshape(1, window, features.shape[1])\n",
        "\n",
        "    pred_lstm = lstm_model.predict(features)[0][0]\n",
        "\n",
        "print(\"Predicted occupancy for Clarence Public School (JP Nagar 4th Phase)\")\n",
        "print(\"Synthetic loc_id used:\", loc_id)\n",
        "print(f\"XGBoost occupancy_rate: {pred_xgb:.3f}\")\n",
        "if has_lstm:\n",
        "    print(f\"LSTM occupancy_rate: {pred_lstm:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqAt-nszEqCk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
